# AUTOGENERATED! DO NOT EDIT! File to edit: 01_modelAPI.ipynb (unless otherwise specified).

__all__ = ['mish', 'Mish', 'EmoModel', 'label2int', 'get_model', 'load_tokenizer', 'setup_tokenizer', 'print_emotion',
           'call_model']

# Cell
# necessary evil
import torch
import torch.nn as nn
import torch.nn.functional as F
import os

# Cell
# from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py
@torch.jit.script
def mish(input):
    return input * torch.tanh(F.softplus(input))

class Mish(nn.Module):
    def forward(self, input):
        return mish(input)

# Cell
class EmoModel(nn.Module):
    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):
        super().__init__()
        self.base_model = base_model

        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(base_model_output_size, base_model_output_size),
            Mish(),
            nn.Dropout(dropout),
            nn.Linear(base_model_output_size, n_classes)
        )

        for layer in self.classifier:
            if isinstance(layer, nn.Linear):
                layer.weight.data.normal_(mean=0.0, std=0.02)
                if layer.bias is not None:
                    layer.bias.data.zero_()

    def forward(self, input_, *args):
        X, attention_mask = input_
        hidden_states = self.base_model(X, attention_mask=attention_mask)
        # customize here
        # use the <s> representation
        return self.classifier(hidden_states[0][:, 0, :])

# Cell
label2int = {
    "sadness": 0,
    "joy": 1,
    "love": 2,
    "anger": 3,
    "fear": 4,
    "surprise": 5
}

# Cell
def get_model(PATH, pretrained=True, inference_only=True, lr=0.0001):
    if PATH[-3:] != ".pt" and PATH[-4:] != ".pth":
        print("Unable to load pretrained model")
        return None

    # show warning message when it's inference only but lr has been changed
    if inference_only == True and lr != 0.0001:
        print("Warning: the loaded model is for inference only, so there's no optimizer for the changed learning rate")
    # model = EmoModel(AutoModelWithLMHead.from_pretrained("distilroberta-base").base_model, len(emotions))
    # see above cell: emotions = ["sadness", "joy", "love", "anger", "fear", "surprise"]
    # len(label2int) would work as well
    from transformers import AutoModelWithLMHead
    model = EmoModel(AutoModelWithLMHead.from_pretrained("distilroberta-base").base_model, 6)

    # if you want to re-train the head
    if pretrained == False:
        return model

    checkpoint = torch.load(PATH)

    if inference_only:
        # model would not be subscriptable
        model.load_state_dict(checkpoint)
        model.eval()
    else:
        # lr: learning rate, adjustable
        from transformers import AdamW
        optimizer = AdamW(model.parameters(), lr=lr)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        model.train()

    return model

# Cell
# save the pretrained token
def load_tokenizer():
    from tokenizers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')
    # -p, --parent, no error if it exists, make parent directory as necessary
    os.system("mkdir -p tokenizer")
    tokenizer.save_pretrained("tokenizer")

def setup_tokenizer():
    # if there's no previous file/record
    # should we check if there are missing files given that it's previously downloaded?
    if not os.path.isdir("tokenizer"):
        load_tokenizer()

    else: # content of previously download files is not complete
        checklist = ['merges.txt', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']
        # check existing files
        # existing_files = os.walk(os.path.join(os.getcwd(), "tokenizer"))
        existing_files = list(os.walk("tokenizer"))[0][2]
        # os.walk() won't messed up the order of the searched files,
        # so, we can just use "==" operator
        if existing_files != checklist:
            # clean the previously download ones
            os.system("rmdir -rf tokenizer")
            # and, re-download it
            load_tokenizer()

# Cell
def print_emotion(output):
    # output = model.forward(input)
    idx = torch.argmax(output, dim=1)
    from .model_api import label2int
    for key in label2int:
        if label2int[key] == idx:
            print("Emotion: %s" % key)
            break
    return key

# Cell
def call_model(text, model_path, inference_only=True):
    model = get_model(model_path, inference_only=inference_only)
    from .data_process import get_tokenizer
    get_tokenizer()
    import torch
    with torch.no_grad():
        model.eval()
        output = model.forward(convert_text_to_tensor(text))
    return output